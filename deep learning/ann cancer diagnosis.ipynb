{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xcN--MH5ZyK6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LqVk8GAFZ_A-"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WbyvwyTQaB84"
      },
      "outputs": [],
      "source": [
        "cancer = load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNYAUke0aIGI",
        "outputId": "7a4ac1c1-b456-43f3-aaed-514e587add76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cancer.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "wzzpViO6aEP9",
        "outputId": "440bc23d-bccd-48ff-fdef-08b156d3ac4c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0        17.99         10.38          122.80     1001.0          0.11840   \n",
              "1        20.57         17.77          132.90     1326.0          0.08474   \n",
              "2        19.69         21.25          130.00     1203.0          0.10960   \n",
              "3        11.42         20.38           77.58      386.1          0.14250   \n",
              "4        20.29         14.34          135.10     1297.0          0.10030   \n",
              "\n",
              "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0           0.27760          0.3001              0.14710         0.2419   \n",
              "1           0.07864          0.0869              0.07017         0.1812   \n",
              "2           0.15990          0.1974              0.12790         0.2069   \n",
              "3           0.28390          0.2414              0.10520         0.2597   \n",
              "4           0.13280          0.1980              0.10430         0.1809   \n",
              "\n",
              "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
              "0                 0.07871  ...         25.38          17.33           184.60   \n",
              "1                 0.05667  ...         24.99          23.41           158.80   \n",
              "2                 0.05999  ...         23.57          25.53           152.50   \n",
              "3                 0.09744  ...         14.91          26.50            98.87   \n",
              "4                 0.05883  ...         22.54          16.67           152.20   \n",
              "\n",
              "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
              "0      2019.0            0.1622             0.6656           0.7119   \n",
              "1      1956.0            0.1238             0.1866           0.2416   \n",
              "2      1709.0            0.1444             0.4245           0.4504   \n",
              "3       567.7            0.2098             0.8663           0.6869   \n",
              "4      1575.0            0.1374             0.2050           0.4000   \n",
              "\n",
              "   worst concave points  worst symmetry  worst fractal dimension  \n",
              "0                0.2654          0.4601                  0.11890  \n",
              "1                0.1860          0.2750                  0.08902  \n",
              "2                0.2430          0.3613                  0.08758  \n",
              "3                0.2575          0.6638                  0.17300  \n",
              "4                0.1625          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(data=cancer['data'], columns=cancer['feature_names'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AjbCRbVbaM8A"
      },
      "outputs": [],
      "source": [
        "df['target'] = cancer['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "56V7oI4taQ7Q",
        "outputId": "db56aa50-a4ff-4db0-a17f-6624c1c075fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0        17.99         10.38          122.80     1001.0          0.11840   \n",
              "1        20.57         17.77          132.90     1326.0          0.08474   \n",
              "2        19.69         21.25          130.00     1203.0          0.10960   \n",
              "3        11.42         20.38           77.58      386.1          0.14250   \n",
              "4        20.29         14.34          135.10     1297.0          0.10030   \n",
              "\n",
              "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0           0.27760          0.3001              0.14710         0.2419   \n",
              "1           0.07864          0.0869              0.07017         0.1812   \n",
              "2           0.15990          0.1974              0.12790         0.2069   \n",
              "3           0.28390          0.2414              0.10520         0.2597   \n",
              "4           0.13280          0.1980              0.10430         0.1809   \n",
              "\n",
              "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
              "0                 0.07871  ...          17.33           184.60      2019.0   \n",
              "1                 0.05667  ...          23.41           158.80      1956.0   \n",
              "2                 0.05999  ...          25.53           152.50      1709.0   \n",
              "3                 0.09744  ...          26.50            98.87       567.7   \n",
              "4                 0.05883  ...          16.67           152.20      1575.0   \n",
              "\n",
              "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
              "0            0.1622             0.6656           0.7119                0.2654   \n",
              "1            0.1238             0.1866           0.2416                0.1860   \n",
              "2            0.1444             0.4245           0.4504                0.2430   \n",
              "3            0.2098             0.8663           0.6869                0.2575   \n",
              "4            0.1374             0.2050           0.4000                0.1625   \n",
              "\n",
              "   worst symmetry  worst fractal dimension  target  \n",
              "0          0.4601                  0.11890       0  \n",
              "1          0.2750                  0.08902       0  \n",
              "2          0.3613                  0.08758       0  \n",
              "3          0.6638                  0.17300       0  \n",
              "4          0.2364                  0.07678       0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W17LeTsIaR7H",
        "outputId": "7bd1e352-3709-4d8b-d2ac-17d8b602287c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 31 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   mean radius              569 non-null    float64\n",
            " 1   mean texture             569 non-null    float64\n",
            " 2   mean perimeter           569 non-null    float64\n",
            " 3   mean area                569 non-null    float64\n",
            " 4   mean smoothness          569 non-null    float64\n",
            " 5   mean compactness         569 non-null    float64\n",
            " 6   mean concavity           569 non-null    float64\n",
            " 7   mean concave points      569 non-null    float64\n",
            " 8   mean symmetry            569 non-null    float64\n",
            " 9   mean fractal dimension   569 non-null    float64\n",
            " 10  radius error             569 non-null    float64\n",
            " 11  texture error            569 non-null    float64\n",
            " 12  perimeter error          569 non-null    float64\n",
            " 13  area error               569 non-null    float64\n",
            " 14  smoothness error         569 non-null    float64\n",
            " 15  compactness error        569 non-null    float64\n",
            " 16  concavity error          569 non-null    float64\n",
            " 17  concave points error     569 non-null    float64\n",
            " 18  symmetry error           569 non-null    float64\n",
            " 19  fractal dimension error  569 non-null    float64\n",
            " 20  worst radius             569 non-null    float64\n",
            " 21  worst texture            569 non-null    float64\n",
            " 22  worst perimeter          569 non-null    float64\n",
            " 23  worst area               569 non-null    float64\n",
            " 24  worst smoothness         569 non-null    float64\n",
            " 25  worst compactness        569 non-null    float64\n",
            " 26  worst concavity          569 non-null    float64\n",
            " 27  worst concave points     569 non-null    float64\n",
            " 28  worst symmetry           569 non-null    float64\n",
            " 29  worst fractal dimension  569 non-null    float64\n",
            " 30  target                   569 non-null    int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 137.9 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5vg1-aLaaSwh"
      },
      "outputs": [],
      "source": [
        "X = df.drop('target', axis=1).values\n",
        "y = df['target'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnZ1VqoSadWP",
        "outputId": "c9607ac4-9e16-4442-9225-bfb4ebd087f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "        1.189e-01],\n",
              "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "        8.902e-02],\n",
              "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "        8.758e-02],\n",
              "       ...,\n",
              "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "        7.820e-02],\n",
              "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "        1.240e-01],\n",
              "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "        7.039e-02]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4FkFfF-ad6I",
        "outputId": "5db6c47e-65f4-48d0-94f6-65da76f5c4cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vQMSzfC6ax85"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wobvWxboaev3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVJNYvmEauCq",
        "outputId": "c2f333c6-207e-4742-b1f7-e219a70872be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MinMaxScaler()"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j1Kbu44Za-hn"
      },
      "outputs": [],
      "source": [
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgmbOQI_bE1x",
        "outputId": "f2752802-4bec-4ec1-e8d4-a2ba22e1c066"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.34402953, 0.42069665, 0.3636929 , ..., 0.61340206, 0.25211906,\n",
              "        0.49304849],\n",
              "       [0.22381561, 0.19411566, 0.21588004, ..., 0.27024055, 0.23654642,\n",
              "        0.1951509 ],\n",
              "       [0.13015287, 0.19039567, 0.13813835, ..., 0.53986254, 0.30415927,\n",
              "        0.60071211],\n",
              "       ...,\n",
              "       [0.23233471, 0.25870815, 0.22396517, ..., 0.30742268, 0.12911492,\n",
              "        0.04484571],\n",
              "       [0.31847224, 0.30334799, 0.31055214, ..., 0.44123711, 0.25310467,\n",
              "        0.29899966],\n",
              "       [0.60670169, 0.400744  , 0.5936701 , ..., 0.62542955, 0.29666864,\n",
              "        0.23321465]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VsQNgigGbFwY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1XOIqPFbTbE",
        "outputId": "4fbe044d-8ced-455d-ee07-5e6c3939274c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(381, 30)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8ON8WWm6bYlR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-01 11:51:44.918874: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(30, activation='relu'))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(7, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zh1cxvicYVb",
        "outputId": "6eaacabd-0fe1-4169-81fb-434ba9a7f971"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-01 11:51:47.641344: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "12/12 [==============================] - 1s 19ms/step - loss: 0.6797 - val_loss: 0.6718\n",
            "Epoch 2/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.6645 - val_loss: 0.6580\n",
            "Epoch 3/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6496 - val_loss: 0.6433\n",
            "Epoch 4/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6329 - val_loss: 0.6255\n",
            "Epoch 5/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6113 - val_loss: 0.6037\n",
            "Epoch 6/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5839 - val_loss: 0.5737\n",
            "Epoch 7/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5500 - val_loss: 0.5396\n",
            "Epoch 8/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.5117 - val_loss: 0.5019\n",
            "Epoch 9/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4717 - val_loss: 0.4583\n",
            "Epoch 10/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4323 - val_loss: 0.4177\n",
            "Epoch 11/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3905 - val_loss: 0.3791\n",
            "Epoch 12/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3512 - val_loss: 0.3410\n",
            "Epoch 13/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3158 - val_loss: 0.3047\n",
            "Epoch 14/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2849 - val_loss: 0.2788\n",
            "Epoch 15/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2587 - val_loss: 0.2548\n",
            "Epoch 16/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2477 - val_loss: 0.2405\n",
            "Epoch 17/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2284 - val_loss: 0.2247\n",
            "Epoch 18/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2121 - val_loss: 0.2129\n",
            "Epoch 19/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2022 - val_loss: 0.1997\n",
            "Epoch 20/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1887 - val_loss: 0.1903\n",
            "Epoch 21/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1777 - val_loss: 0.1831\n",
            "Epoch 22/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1695 - val_loss: 0.1753\n",
            "Epoch 23/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1630 - val_loss: 0.1680\n",
            "Epoch 24/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1557 - val_loss: 0.1624\n",
            "Epoch 25/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1475 - val_loss: 0.1562\n",
            "Epoch 26/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1429 - val_loss: 0.1506\n",
            "Epoch 27/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1395 - val_loss: 0.1448\n",
            "Epoch 28/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1305 - val_loss: 0.1415\n",
            "Epoch 29/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1253 - val_loss: 0.1358\n",
            "Epoch 30/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1214 - val_loss: 0.1319\n",
            "Epoch 31/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1167 - val_loss: 0.1299\n",
            "Epoch 32/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1120 - val_loss: 0.1258\n",
            "Epoch 33/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1120 - val_loss: 0.1210\n",
            "Epoch 34/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1035 - val_loss: 0.1162\n",
            "Epoch 35/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1000 - val_loss: 0.1128\n",
            "Epoch 36/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0981 - val_loss: 0.1110\n",
            "Epoch 37/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0935 - val_loss: 0.1078\n",
            "Epoch 38/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0909 - val_loss: 0.1051\n",
            "Epoch 39/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0883 - val_loss: 0.1018\n",
            "Epoch 40/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0916 - val_loss: 0.1020\n",
            "Epoch 41/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0824 - val_loss: 0.0974\n",
            "Epoch 42/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0830 - val_loss: 0.0986\n",
            "Epoch 43/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0767 - val_loss: 0.0942\n",
            "Epoch 44/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0772 - val_loss: 0.0977\n",
            "Epoch 45/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0741 - val_loss: 0.0899\n",
            "Epoch 46/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0712 - val_loss: 0.0901\n",
            "Epoch 47/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0708 - val_loss: 0.0872\n",
            "Epoch 48/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0677 - val_loss: 0.0887\n",
            "Epoch 49/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0842\n",
            "Epoch 50/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0666 - val_loss: 0.0844\n",
            "Epoch 51/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0648 - val_loss: 0.0828\n",
            "Epoch 52/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0635 - val_loss: 0.0807\n",
            "Epoch 53/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0636 - val_loss: 0.0795\n",
            "Epoch 54/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0600 - val_loss: 0.0810\n",
            "Epoch 55/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0635 - val_loss: 0.0805\n",
            "Epoch 56/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0586 - val_loss: 0.0771\n",
            "Epoch 57/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.0759\n",
            "Epoch 58/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0556 - val_loss: 0.0810\n",
            "Epoch 59/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0557 - val_loss: 0.0756\n",
            "Epoch 60/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0567 - val_loss: 0.0797\n",
            "Epoch 61/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0572 - val_loss: 0.0728\n",
            "Epoch 62/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0521 - val_loss: 0.0738\n",
            "Epoch 63/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0541 - val_loss: 0.0719\n",
            "Epoch 64/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0506 - val_loss: 0.0747\n",
            "Epoch 65/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0507 - val_loss: 0.0704\n",
            "Epoch 66/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0510 - val_loss: 0.0722\n",
            "Epoch 67/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0493 - val_loss: 0.0693\n",
            "Epoch 68/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0477 - val_loss: 0.0696\n",
            "Epoch 69/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0467 - val_loss: 0.0689\n",
            "Epoch 70/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0482 - val_loss: 0.0683\n",
            "Epoch 71/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0462 - val_loss: 0.0681\n",
            "Epoch 72/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0456 - val_loss: 0.0705\n",
            "Epoch 73/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0478 - val_loss: 0.0670\n",
            "Epoch 74/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0449 - val_loss: 0.0666\n",
            "Epoch 75/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.0661\n",
            "Epoch 76/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0453 - val_loss: 0.0693\n",
            "Epoch 77/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0447 - val_loss: 0.0660\n",
            "Epoch 78/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0430 - val_loss: 0.0653\n",
            "Epoch 79/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0443 - val_loss: 0.0683\n",
            "Epoch 80/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.0644\n",
            "Epoch 81/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0423 - val_loss: 0.0674\n",
            "Epoch 82/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0428 - val_loss: 0.0642\n",
            "Epoch 83/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0402 - val_loss: 0.0645\n",
            "Epoch 84/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0432 - val_loss: 0.0647\n",
            "Epoch 85/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0406 - val_loss: 0.0644\n",
            "Epoch 86/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0393 - val_loss: 0.0641\n",
            "Epoch 87/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0398 - val_loss: 0.0638\n",
            "Epoch 88/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0378 - val_loss: 0.0696\n",
            "Epoch 89/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0390 - val_loss: 0.0636\n",
            "Epoch 90/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0403 - val_loss: 0.0712\n",
            "Epoch 91/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0389 - val_loss: 0.0628\n",
            "Epoch 92/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0373 - val_loss: 0.0655\n",
            "Epoch 93/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0363 - val_loss: 0.0642\n",
            "Epoch 94/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0357 - val_loss: 0.0658\n",
            "Epoch 95/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0398 - val_loss: 0.0628\n",
            "Epoch 96/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0385 - val_loss: 0.0631\n",
            "Epoch 97/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0436 - val_loss: 0.0651\n",
            "Epoch 98/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0386 - val_loss: 0.0675\n",
            "Epoch 99/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0696\n",
            "Epoch 100/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0366 - val_loss: 0.0694\n",
            "Epoch 101/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0701\n",
            "Epoch 102/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0368 - val_loss: 0.0667\n",
            "Epoch 103/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0338 - val_loss: 0.0679\n",
            "Epoch 104/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0326 - val_loss: 0.0691\n",
            "Epoch 105/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0325 - val_loss: 0.0687\n",
            "Epoch 106/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0351 - val_loss: 0.0666\n",
            "Epoch 107/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.0706\n",
            "Epoch 108/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0325 - val_loss: 0.0673\n",
            "Epoch 109/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0321 - val_loss: 0.0688\n",
            "Epoch 110/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0319 - val_loss: 0.0717\n",
            "Epoch 111/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0315 - val_loss: 0.0736\n",
            "Epoch 112/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0311 - val_loss: 0.0722\n",
            "Epoch 113/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0305 - val_loss: 0.0701\n",
            "Epoch 114/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0304 - val_loss: 0.0688\n",
            "Epoch 115/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0325 - val_loss: 0.0748\n",
            "Epoch 116/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0311 - val_loss: 0.0724\n",
            "Epoch 117/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0304 - val_loss: 0.0718\n",
            "Epoch 118/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0320 - val_loss: 0.0735\n",
            "Epoch 119/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0339 - val_loss: 0.0712\n",
            "Epoch 120/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0294 - val_loss: 0.0781\n",
            "Epoch 121/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0298 - val_loss: 0.0789\n",
            "Epoch 122/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0284 - val_loss: 0.0683\n",
            "Epoch 123/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.0784\n",
            "Epoch 124/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0295 - val_loss: 0.0699\n",
            "Epoch 125/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0299 - val_loss: 0.0828\n",
            "Epoch 126/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0321 - val_loss: 0.0722\n",
            "Epoch 127/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0838\n",
            "Epoch 128/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.0754\n",
            "Epoch 129/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0801\n",
            "Epoch 130/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0262 - val_loss: 0.0776\n",
            "Epoch 131/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0257 - val_loss: 0.0767\n",
            "Epoch 132/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0772\n",
            "Epoch 133/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0253 - val_loss: 0.0762\n",
            "Epoch 134/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0254 - val_loss: 0.0835\n",
            "Epoch 135/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0249 - val_loss: 0.0756\n",
            "Epoch 136/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0247 - val_loss: 0.0837\n",
            "Epoch 137/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0250 - val_loss: 0.0834\n",
            "Epoch 138/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - val_loss: 0.0813\n",
            "Epoch 139/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - val_loss: 0.0807\n",
            "Epoch 140/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0249 - val_loss: 0.0823\n",
            "Epoch 141/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0236 - val_loss: 0.0815\n",
            "Epoch 142/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.0868\n",
            "Epoch 143/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0244 - val_loss: 0.0830\n",
            "Epoch 144/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0233 - val_loss: 0.0879\n",
            "Epoch 145/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.0824\n",
            "Epoch 146/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0238 - val_loss: 0.0878\n",
            "Epoch 147/600\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0254 - val_loss: 0.0931\n",
            "Epoch 148/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0267 - val_loss: 0.0802\n",
            "Epoch 149/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0244 - val_loss: 0.1044\n",
            "Epoch 150/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0239 - val_loss: 0.0813\n",
            "Epoch 151/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - val_loss: 0.1052\n",
            "Epoch 152/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0237 - val_loss: 0.0815\n",
            "Epoch 153/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - val_loss: 0.0906\n",
            "Epoch 154/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0224 - val_loss: 0.0844\n",
            "Epoch 155/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0226 - val_loss: 0.0888\n",
            "Epoch 156/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0206 - val_loss: 0.0871\n",
            "Epoch 157/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.0898\n",
            "Epoch 158/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0234 - val_loss: 0.0970\n",
            "Epoch 159/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - val_loss: 0.0905\n",
            "Epoch 160/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0194 - val_loss: 0.0900\n",
            "Epoch 161/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0200 - val_loss: 0.0952\n",
            "Epoch 162/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - val_loss: 0.0921\n",
            "Epoch 163/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0194 - val_loss: 0.0982\n",
            "Epoch 164/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0219 - val_loss: 0.1089\n",
            "Epoch 165/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0204 - val_loss: 0.0889\n",
            "Epoch 166/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.1022\n",
            "Epoch 167/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - val_loss: 0.0909\n",
            "Epoch 168/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0193 - val_loss: 0.1021\n",
            "Epoch 169/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 0.0926\n",
            "Epoch 170/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0185 - val_loss: 0.1016\n",
            "Epoch 171/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0179 - val_loss: 0.0971\n",
            "Epoch 172/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.0982\n",
            "Epoch 173/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0189 - val_loss: 0.0934\n",
            "Epoch 174/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0179 - val_loss: 0.1040\n",
            "Epoch 175/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - val_loss: 0.1038\n",
            "Epoch 176/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - val_loss: 0.0979\n",
            "Epoch 177/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0165 - val_loss: 0.1062\n",
            "Epoch 178/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0189 - val_loss: 0.1039\n",
            "Epoch 179/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0169 - val_loss: 0.1030\n",
            "Epoch 180/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0989\n",
            "Epoch 181/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.1088\n",
            "Epoch 182/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.1069\n",
            "Epoch 183/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0179 - val_loss: 0.1078\n",
            "Epoch 184/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.0989\n",
            "Epoch 185/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0175 - val_loss: 0.1302\n",
            "Epoch 186/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.0948\n",
            "Epoch 187/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0213 - val_loss: 0.1103\n",
            "Epoch 188/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0178 - val_loss: 0.1124\n",
            "Epoch 189/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.1022\n",
            "Epoch 190/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0154 - val_loss: 0.1132\n",
            "Epoch 191/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0156 - val_loss: 0.1120\n",
            "Epoch 192/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0156 - val_loss: 0.1065\n",
            "Epoch 193/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1116\n",
            "Epoch 194/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.1126\n",
            "Epoch 195/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.1064\n",
            "Epoch 196/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.1224\n",
            "Epoch 197/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.1067\n",
            "Epoch 198/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.1174\n",
            "Epoch 199/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.1154\n",
            "Epoch 200/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.1125\n",
            "Epoch 201/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.1153\n",
            "Epoch 202/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.1194\n",
            "Epoch 203/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0157 - val_loss: 0.1087\n",
            "Epoch 204/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.1203\n",
            "Epoch 205/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.1119\n",
            "Epoch 206/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.1404\n",
            "Epoch 207/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0195 - val_loss: 0.1074\n",
            "Epoch 208/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0137 - val_loss: 0.1130\n",
            "Epoch 209/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.1136\n",
            "Epoch 210/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.1300\n",
            "Epoch 211/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0153 - val_loss: 0.1234\n",
            "Epoch 212/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.1195\n",
            "Epoch 213/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.1161\n",
            "Epoch 214/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.1230\n",
            "Epoch 215/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.1317\n",
            "Epoch 216/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0145 - val_loss: 0.1173\n",
            "Epoch 217/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.1261\n",
            "Epoch 218/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.1194\n",
            "Epoch 219/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.1308\n",
            "Epoch 220/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - val_loss: 0.1260\n",
            "Epoch 221/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - val_loss: 0.1275\n",
            "Epoch 222/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0110 - val_loss: 0.1348\n",
            "Epoch 223/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - val_loss: 0.1220\n",
            "Epoch 224/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0116 - val_loss: 0.1310\n",
            "Epoch 225/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0112 - val_loss: 0.1308\n",
            "Epoch 226/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.1254\n",
            "Epoch 227/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.1318\n",
            "Epoch 228/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.1325\n",
            "Epoch 229/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.1299\n",
            "Epoch 230/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.1274\n",
            "Epoch 231/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0105 - val_loss: 0.1316\n",
            "Epoch 232/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - val_loss: 0.1388\n",
            "Epoch 233/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0131 - val_loss: 0.1394\n",
            "Epoch 234/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.1268\n",
            "Epoch 235/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.1369\n",
            "Epoch 236/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - val_loss: 0.1287\n",
            "Epoch 237/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - val_loss: 0.1337\n",
            "Epoch 238/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0090 - val_loss: 0.1400\n",
            "Epoch 239/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.1326\n",
            "Epoch 240/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0116 - val_loss: 0.1425\n",
            "Epoch 241/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0126 - val_loss: 0.1422\n",
            "Epoch 242/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0175 - val_loss: 0.1332\n",
            "Epoch 243/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0087 - val_loss: 0.1552\n",
            "Epoch 244/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.1337\n",
            "Epoch 245/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0087 - val_loss: 0.1456\n",
            "Epoch 246/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.1392\n",
            "Epoch 247/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0088 - val_loss: 0.1389\n",
            "Epoch 248/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.1405\n",
            "Epoch 249/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.1441\n",
            "Epoch 250/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0088 - val_loss: 0.1406\n",
            "Epoch 251/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0088 - val_loss: 0.1404\n",
            "Epoch 252/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0086 - val_loss: 0.1434\n",
            "Epoch 253/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0094 - val_loss: 0.1406\n",
            "Epoch 254/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0083 - val_loss: 0.1546\n",
            "Epoch 255/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.1357\n",
            "Epoch 256/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0127 - val_loss: 0.1464\n",
            "Epoch 257/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0086 - val_loss: 0.1443\n",
            "Epoch 258/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0080 - val_loss: 0.1457\n",
            "Epoch 259/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0082 - val_loss: 0.1465\n",
            "Epoch 260/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0080 - val_loss: 0.1455\n",
            "Epoch 261/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0072 - val_loss: 0.1435\n",
            "Epoch 262/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0092 - val_loss: 0.1457\n",
            "Epoch 263/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0081 - val_loss: 0.1507\n",
            "Epoch 264/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0075 - val_loss: 0.1470\n",
            "Epoch 265/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0068 - val_loss: 0.1483\n",
            "Epoch 266/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0070 - val_loss: 0.1555\n",
            "Epoch 267/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.1424\n",
            "Epoch 268/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0070 - val_loss: 0.1509\n",
            "Epoch 269/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0080 - val_loss: 0.1516\n",
            "Epoch 270/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.1693\n",
            "Epoch 271/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.1418\n",
            "Epoch 272/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0075 - val_loss: 0.1495\n",
            "Epoch 273/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0070 - val_loss: 0.1512\n",
            "Epoch 274/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0062 - val_loss: 0.1531\n",
            "Epoch 275/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0069 - val_loss: 0.1571\n",
            "Epoch 276/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0077 - val_loss: 0.1497\n",
            "Epoch 277/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0078 - val_loss: 0.1500\n",
            "Epoch 278/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0063 - val_loss: 0.1533\n",
            "Epoch 279/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0067 - val_loss: 0.1531\n",
            "Epoch 280/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0078 - val_loss: 0.1611\n",
            "Epoch 281/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0083 - val_loss: 0.1504\n",
            "Epoch 282/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.1575\n",
            "Epoch 283/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0073 - val_loss: 0.1528\n",
            "Epoch 284/600\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.1553\n",
            "Epoch 285/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0064 - val_loss: 0.1562\n",
            "Epoch 286/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0060 - val_loss: 0.1617\n",
            "Epoch 287/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0071 - val_loss: 0.1525\n",
            "Epoch 288/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0061 - val_loss: 0.1540\n",
            "Epoch 289/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0059 - val_loss: 0.1573\n",
            "Epoch 290/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.1591\n",
            "Epoch 291/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0074 - val_loss: 0.1601\n",
            "Epoch 292/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0104 - val_loss: 0.1536\n",
            "Epoch 293/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0085 - val_loss: 0.1582\n",
            "Epoch 294/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0052 - val_loss: 0.1599\n",
            "Epoch 295/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0061 - val_loss: 0.1612\n",
            "Epoch 296/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.1568\n",
            "Epoch 297/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0059 - val_loss: 0.1591\n",
            "Epoch 298/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0053 - val_loss: 0.1613\n",
            "Epoch 299/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0058 - val_loss: 0.1567\n",
            "Epoch 300/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0062 - val_loss: 0.1551\n",
            "Epoch 301/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0052 - val_loss: 0.1599\n",
            "Epoch 302/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.1630\n",
            "Epoch 303/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0053 - val_loss: 0.1613\n",
            "Epoch 304/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0055 - val_loss: 0.1614\n",
            "Epoch 305/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.1645\n",
            "Epoch 306/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0053 - val_loss: 0.1595\n",
            "Epoch 307/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0051 - val_loss: 0.1606\n",
            "Epoch 308/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0057 - val_loss: 0.1635\n",
            "Epoch 309/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0051 - val_loss: 0.1656\n",
            "Epoch 310/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.1651\n",
            "Epoch 311/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.1647\n",
            "Epoch 312/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.1656\n",
            "Epoch 313/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.1707\n",
            "Epoch 314/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.1613\n",
            "Epoch 315/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0049 - val_loss: 0.1627\n",
            "Epoch 316/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.1680\n",
            "Epoch 317/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.1689\n",
            "Epoch 318/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.1680\n",
            "Epoch 319/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.1735\n",
            "Epoch 320/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0057 - val_loss: 0.1630\n",
            "Epoch 321/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.1679\n",
            "Epoch 322/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.1693\n",
            "Epoch 323/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.1723\n",
            "Epoch 324/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.1675\n",
            "Epoch 325/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0035 - val_loss: 0.1712\n",
            "Epoch 326/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0063 - val_loss: 0.1684\n",
            "Epoch 327/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.1726\n",
            "Epoch 328/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.1704\n",
            "Epoch 329/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.1687\n",
            "Epoch 330/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0049 - val_loss: 0.1698\n",
            "Epoch 331/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0055 - val_loss: 0.1754\n",
            "Epoch 332/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0041 - val_loss: 0.1737\n",
            "Epoch 333/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0042 - val_loss: 0.1752\n",
            "Epoch 334/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0042 - val_loss: 0.1719\n",
            "Epoch 335/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0041 - val_loss: 0.1732\n",
            "Epoch 336/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.1762\n",
            "Epoch 337/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.1752\n",
            "Epoch 338/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.1800\n",
            "Epoch 339/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.1776\n",
            "Epoch 340/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.1803\n",
            "Epoch 341/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0039 - val_loss: 0.1732\n",
            "Epoch 342/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0037 - val_loss: 0.1741\n",
            "Epoch 343/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.1767\n",
            "Epoch 344/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.1759\n",
            "Epoch 345/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0037 - val_loss: 0.1765\n",
            "Epoch 346/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0050 - val_loss: 0.1793\n",
            "Epoch 347/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.1806\n",
            "Epoch 348/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0036 - val_loss: 0.1785\n",
            "Epoch 349/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.1803\n",
            "Epoch 350/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0060 - val_loss: 0.1810\n",
            "Epoch 351/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0046 - val_loss: 0.1759\n",
            "Epoch 352/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0042 - val_loss: 0.1798\n",
            "Epoch 353/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0054 - val_loss: 0.1759\n",
            "Epoch 354/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.1812\n",
            "Epoch 355/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0034 - val_loss: 0.1796\n",
            "Epoch 356/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.1801\n",
            "Epoch 357/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0029 - val_loss: 0.1810\n",
            "Epoch 358/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.1824\n",
            "Epoch 359/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.1835\n",
            "Epoch 360/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 0.1802\n",
            "Epoch 361/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.1814\n",
            "Epoch 362/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 0.1857\n",
            "Epoch 363/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.1812\n",
            "Epoch 364/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.1815\n",
            "Epoch 365/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0030 - val_loss: 0.1841\n",
            "Epoch 366/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.1875\n",
            "Epoch 367/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0027 - val_loss: 0.1853\n",
            "Epoch 368/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 0.1837\n",
            "Epoch 369/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 0.1866\n",
            "Epoch 370/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0032 - val_loss: 0.1849\n",
            "Epoch 371/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0033 - val_loss: 0.1857\n",
            "Epoch 372/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0045 - val_loss: 0.1914\n",
            "Epoch 373/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0036 - val_loss: 0.1908\n",
            "Epoch 374/600\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.1837\n",
            "Epoch 375/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0039 - val_loss: 0.1827\n",
            "Epoch 376/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0051 - val_loss: 0.1973\n",
            "Epoch 377/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.1885\n",
            "Epoch 378/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0023 - val_loss: 0.1914\n",
            "Epoch 379/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.1906\n",
            "Epoch 380/600\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.1937\n",
            "Epoch 381/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0023 - val_loss: 0.1918\n",
            "Epoch 382/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0029 - val_loss: 0.1883\n",
            "Epoch 383/600\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0027 - val_loss: 0.1877\n",
            "Epoch 384/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0024 - val_loss: 0.1901\n",
            "Epoch 385/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0023 - val_loss: 0.1916\n",
            "Epoch 386/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.1920\n",
            "Epoch 387/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0027 - val_loss: 0.1941\n",
            "Epoch 388/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.1872\n",
            "Epoch 389/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0048 - val_loss: 0.1982\n",
            "Epoch 390/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.1923\n",
            "Epoch 391/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0030 - val_loss: 0.1884\n",
            "Epoch 392/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.1904\n",
            "Epoch 393/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.1934\n",
            "Epoch 394/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 0.1957\n",
            "Epoch 395/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 0.1953\n",
            "Epoch 396/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.1991\n",
            "Epoch 397/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0031 - val_loss: 0.1988\n",
            "Epoch 398/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0028 - val_loss: 0.1968\n",
            "Epoch 399/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.1956\n",
            "Epoch 400/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0025 - val_loss: 0.1963\n",
            "Epoch 401/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.1971\n",
            "Epoch 402/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0019 - val_loss: 0.2004\n",
            "Epoch 403/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0021 - val_loss: 0.1968\n",
            "Epoch 404/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.1953\n",
            "Epoch 405/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0018 - val_loss: 0.1981\n",
            "Epoch 406/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0026 - val_loss: 0.2005\n",
            "Epoch 407/600\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0025 - val_loss: 0.1985\n",
            "Epoch 408/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0019 - val_loss: 0.1951\n",
            "Epoch 409/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0019 - val_loss: 0.1972\n",
            "Epoch 410/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0018 - val_loss: 0.1994\n",
            "Epoch 411/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2023\n",
            "Epoch 412/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 0.1971\n",
            "Epoch 413/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0020 - val_loss: 0.1982\n",
            "Epoch 414/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.2034\n",
            "Epoch 415/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.2007\n",
            "Epoch 416/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0020 - val_loss: 0.2003\n",
            "Epoch 417/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0019 - val_loss: 0.2002\n",
            "Epoch 418/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.2022\n",
            "Epoch 419/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2055\n",
            "Epoch 420/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0021 - val_loss: 0.2004\n",
            "Epoch 421/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 0.2004\n",
            "Epoch 422/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2040\n",
            "Epoch 423/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 0.2045\n",
            "Epoch 424/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2029\n",
            "Epoch 425/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0017 - val_loss: 0.2057\n",
            "Epoch 426/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.2068\n",
            "Epoch 427/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2074\n",
            "Epoch 428/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 0.2002\n",
            "Epoch 429/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2040\n",
            "Epoch 430/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 0.2101\n",
            "Epoch 431/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2123\n",
            "Epoch 432/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0021 - val_loss: 0.1994\n",
            "Epoch 433/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 0.1991\n",
            "Epoch 434/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0032 - val_loss: 0.2090\n",
            "Epoch 435/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0040 - val_loss: 0.2299\n",
            "Epoch 436/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.2115\n",
            "Epoch 437/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0023 - val_loss: 0.2131\n",
            "Epoch 438/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0018 - val_loss: 0.2116\n",
            "Epoch 439/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.2077\n",
            "Epoch 440/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2027\n",
            "Epoch 441/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.2043\n",
            "Epoch 442/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0017 - val_loss: 0.2086\n",
            "Epoch 443/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0017 - val_loss: 0.2113\n",
            "Epoch 444/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0015 - val_loss: 0.2132\n",
            "Epoch 445/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.2115\n",
            "Epoch 446/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.2105\n",
            "Epoch 447/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.2117\n",
            "Epoch 448/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.2133\n",
            "Epoch 449/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0013 - val_loss: 0.2121\n",
            "Epoch 450/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.2124\n",
            "Epoch 451/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.2163\n",
            "Epoch 452/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.2131\n",
            "Epoch 453/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.2126\n",
            "Epoch 454/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0015 - val_loss: 0.2140\n",
            "Epoch 455/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0013 - val_loss: 0.2165\n",
            "Epoch 456/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.2144\n",
            "Epoch 457/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0012 - val_loss: 0.2154\n",
            "Epoch 458/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.2177\n",
            "Epoch 459/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0013 - val_loss: 0.2135\n",
            "Epoch 460/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.2135\n",
            "Epoch 461/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.2155\n",
            "Epoch 462/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.2225\n",
            "Epoch 463/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.2146\n",
            "Epoch 464/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.2145\n",
            "Epoch 465/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.2177\n",
            "Epoch 466/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2205\n",
            "Epoch 467/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.2213\n",
            "Epoch 468/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.2182\n",
            "Epoch 469/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.2186\n",
            "Epoch 470/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2214\n",
            "Epoch 471/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2241\n",
            "Epoch 472/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.2208\n",
            "Epoch 473/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.2189\n",
            "Epoch 474/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.2226\n",
            "Epoch 475/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.2252\n",
            "Epoch 476/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2255\n",
            "Epoch 477/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2188\n",
            "Epoch 478/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.2203\n",
            "Epoch 479/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 0.2224\n",
            "Epoch 480/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 0.2249\n",
            "Epoch 481/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0010 - val_loss: 0.2271\n",
            "Epoch 482/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.2231\n",
            "Epoch 483/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.2188\n",
            "Epoch 484/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.2209\n",
            "Epoch 485/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.2308\n",
            "Epoch 486/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0022 - val_loss: 0.2306\n",
            "Epoch 487/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0014 - val_loss: 0.2261\n",
            "Epoch 488/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.2145\n",
            "Epoch 489/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0010 - val_loss: 0.2210\n",
            "Epoch 490/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2222\n",
            "Epoch 491/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.2280\n",
            "Epoch 492/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2298\n",
            "Epoch 493/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.5901e-04 - val_loss: 0.2315\n",
            "Epoch 494/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6428e-04 - val_loss: 0.2320\n",
            "Epoch 495/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3310e-04 - val_loss: 0.2326\n",
            "Epoch 496/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4987e-04 - val_loss: 0.2328\n",
            "Epoch 497/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2292\n",
            "Epoch 498/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5673e-04 - val_loss: 0.2304\n",
            "Epoch 499/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.2316\n",
            "Epoch 500/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.2366\n",
            "Epoch 501/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.2364\n",
            "Epoch 502/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.2367\n",
            "Epoch 503/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.2336\n",
            "Epoch 504/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4164e-04 - val_loss: 0.2353\n",
            "Epoch 505/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.2308\n",
            "Epoch 506/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9172e-04 - val_loss: 0.2308\n",
            "Epoch 507/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6053e-04 - val_loss: 0.2338\n",
            "Epoch 508/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6718e-04 - val_loss: 0.2366\n",
            "Epoch 509/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9121e-04 - val_loss: 0.2373\n",
            "Epoch 510/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9108e-04 - val_loss: 0.2356\n",
            "Epoch 511/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.7024e-04 - val_loss: 0.2368\n",
            "Epoch 512/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2151e-04 - val_loss: 0.2384\n",
            "Epoch 513/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7099e-04 - val_loss: 0.2323\n",
            "Epoch 514/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4497e-04 - val_loss: 0.2339\n",
            "Epoch 515/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2357\n",
            "Epoch 516/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1707e-04 - val_loss: 0.2374\n",
            "Epoch 517/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9262e-04 - val_loss: 0.2344\n",
            "Epoch 518/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2666e-04 - val_loss: 0.2363\n",
            "Epoch 519/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.2470\n",
            "Epoch 520/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0025 - val_loss: 0.2486\n",
            "Epoch 521/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0026 - val_loss: 0.2379\n",
            "Epoch 522/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.2428\n",
            "Epoch 523/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0047 - val_loss: 0.2519\n",
            "Epoch 524/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0024 - val_loss: 0.2326\n",
            "Epoch 525/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.2279\n",
            "Epoch 526/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0011 - val_loss: 0.2308\n",
            "Epoch 527/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4481e-04 - val_loss: 0.2356\n",
            "Epoch 528/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8614e-04 - val_loss: 0.2394\n",
            "Epoch 529/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6455e-04 - val_loss: 0.2412\n",
            "Epoch 530/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6400e-04 - val_loss: 0.2423\n",
            "Epoch 531/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1074e-04 - val_loss: 0.2436\n",
            "Epoch 532/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.5251e-04 - val_loss: 0.2411\n",
            "Epoch 533/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6207e-04 - val_loss: 0.2411\n",
            "Epoch 534/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.0845e-04 - val_loss: 0.2427\n",
            "Epoch 535/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4750e-04 - val_loss: 0.2446\n",
            "Epoch 536/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4289e-04 - val_loss: 0.2447\n",
            "Epoch 537/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2982e-04 - val_loss: 0.2457\n",
            "Epoch 538/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0647e-04 - val_loss: 0.2439\n",
            "Epoch 539/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2480\n",
            "Epoch 540/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0012 - val_loss: 0.2472\n",
            "Epoch 541/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0014 - val_loss: 0.2346\n",
            "Epoch 542/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5812e-04 - val_loss: 0.2303\n",
            "Epoch 543/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.2429\n",
            "Epoch 544/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0011 - val_loss: 0.2478\n",
            "Epoch 545/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0016 - val_loss: 0.2345\n",
            "Epoch 546/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0018 - val_loss: 0.2371\n",
            "Epoch 547/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4959e-04 - val_loss: 0.2443\n",
            "Epoch 548/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4225e-04 - val_loss: 0.2479\n",
            "Epoch 549/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7423e-04 - val_loss: 0.2503\n",
            "Epoch 550/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7241e-04 - val_loss: 0.2508\n",
            "Epoch 551/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5573e-04 - val_loss: 0.2507\n",
            "Epoch 552/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1192e-04 - val_loss: 0.2548\n",
            "Epoch 553/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6636e-04 - val_loss: 0.2488\n",
            "Epoch 554/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.5251e-04 - val_loss: 0.2523\n",
            "Epoch 555/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7624e-04 - val_loss: 0.2514\n",
            "Epoch 556/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.9871e-04 - val_loss: 0.2518\n",
            "Epoch 557/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3266e-04 - val_loss: 0.2501\n",
            "Epoch 558/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0216e-04 - val_loss: 0.2487\n",
            "Epoch 559/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8263e-04 - val_loss: 0.2505\n",
            "Epoch 560/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0266e-04 - val_loss: 0.2517\n",
            "Epoch 561/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0256e-04 - val_loss: 0.2532\n",
            "Epoch 562/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8897e-04 - val_loss: 0.2541\n",
            "Epoch 563/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.9402e-04 - val_loss: 0.2544\n",
            "Epoch 564/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8644e-04 - val_loss: 0.2539\n",
            "Epoch 565/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.4775e-04 - val_loss: 0.2474\n",
            "Epoch 566/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6003e-04 - val_loss: 0.2519\n",
            "Epoch 567/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.7619e-04 - val_loss: 0.2508\n",
            "Epoch 568/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6234e-04 - val_loss: 0.2549\n",
            "Epoch 569/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9656e-04 - val_loss: 0.2537\n",
            "Epoch 570/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.7293e-04 - val_loss: 0.2539\n",
            "Epoch 571/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6403e-04 - val_loss: 0.2521\n",
            "Epoch 572/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9553e-04 - val_loss: 0.2518\n",
            "Epoch 573/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0099e-04 - val_loss: 0.2491\n",
            "Epoch 574/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1675e-04 - val_loss: 0.2500\n",
            "Epoch 575/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8179e-04 - val_loss: 0.2544\n",
            "Epoch 576/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.8768e-04 - val_loss: 0.2545\n",
            "Epoch 577/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.4997e-04 - val_loss: 0.2562\n",
            "Epoch 578/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0476e-04 - val_loss: 0.2577\n",
            "Epoch 579/600\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4.9251e-04 - val_loss: 0.2520\n",
            "Epoch 580/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3345e-04 - val_loss: 0.2568\n",
            "Epoch 581/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9257e-04 - val_loss: 0.2545\n",
            "Epoch 582/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.6143e-04 - val_loss: 0.2562\n",
            "Epoch 583/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.4205e-04 - val_loss: 0.2592\n",
            "Epoch 584/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.1482e-04 - val_loss: 0.2583\n",
            "Epoch 585/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.0064e-04 - val_loss: 0.2577\n",
            "Epoch 586/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.9762e-04 - val_loss: 0.2563\n",
            "Epoch 587/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.0936e-04 - val_loss: 0.2556\n",
            "Epoch 588/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.3692e-04 - val_loss: 0.2583\n",
            "Epoch 589/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.8517e-04 - val_loss: 0.2593\n",
            "Epoch 590/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.8548e-04 - val_loss: 0.2580\n",
            "Epoch 591/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.8246e-04 - val_loss: 0.2604\n",
            "Epoch 592/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9454e-04 - val_loss: 0.2534\n",
            "Epoch 593/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.8532e-04 - val_loss: 0.2567\n",
            "Epoch 594/600\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 4.2397e-04 - val_loss: 0.2606\n",
            "Epoch 595/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.0673e-04 - val_loss: 0.2576\n",
            "Epoch 596/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.9349e-04 - val_loss: 0.2609\n",
            "Epoch 597/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.6711e-04 - val_loss: 0.2606\n",
            "Epoch 598/600\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.2435e-04 - val_loss: 0.2600\n",
            "Epoch 599/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.4776e-04 - val_loss: 0.2555\n",
            "Epoch 600/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.9786e-04 - val_loss: 0.2578\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fefb1f1fbb0>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=600, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1Vf_-RCic3eH",
        "outputId": "b9f68219-ceee-4ab3-8976-df097329689f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "lossDf = pd.DataFrame(model.history.history)\n",
        "lossDf.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "S9Ol1K-IfPTy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "earlyStop = EarlyStopping(patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KLvCd4KeluhH"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(30, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIZaQQTulz1l",
        "outputId": "70d04132-f6b3-4cef-eaa2-94855682f5f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "12/12 [==============================] - 1s 12ms/step - loss: 0.7198 - val_loss: 0.6754\n",
            "Epoch 2/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6652 - val_loss: 0.6677\n",
            "Epoch 3/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6741 - val_loss: 0.6606\n",
            "Epoch 4/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6733 - val_loss: 0.6556\n",
            "Epoch 5/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6790 - val_loss: 0.6532\n",
            "Epoch 6/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6353 - val_loss: 0.6428\n",
            "Epoch 7/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6542 - val_loss: 0.6322\n",
            "Epoch 8/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6309 - val_loss: 0.6189\n",
            "Epoch 9/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6248 - val_loss: 0.6033\n",
            "Epoch 10/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.6348 - val_loss: 0.5930\n",
            "Epoch 11/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.6040 - val_loss: 0.5788\n",
            "Epoch 12/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5942 - val_loss: 0.5623\n",
            "Epoch 13/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5733 - val_loss: 0.5370\n",
            "Epoch 14/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.5808 - val_loss: 0.5200\n",
            "Epoch 15/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5578 - val_loss: 0.5077\n",
            "Epoch 16/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5518 - val_loss: 0.4913\n",
            "Epoch 17/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5501 - val_loss: 0.4740\n",
            "Epoch 18/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5489 - val_loss: 0.4605\n",
            "Epoch 19/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4857 - val_loss: 0.4333\n",
            "Epoch 20/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4832 - val_loss: 0.4106\n",
            "Epoch 21/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4908 - val_loss: 0.3866\n",
            "Epoch 22/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4547 - val_loss: 0.3716\n",
            "Epoch 23/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4494 - val_loss: 0.3556\n",
            "Epoch 24/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4278 - val_loss: 0.3368\n",
            "Epoch 25/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4686 - val_loss: 0.3247\n",
            "Epoch 26/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4180 - val_loss: 0.3151\n",
            "Epoch 27/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4415 - val_loss: 0.3090\n",
            "Epoch 28/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4204 - val_loss: 0.3036\n",
            "Epoch 29/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4441 - val_loss: 0.3004\n",
            "Epoch 30/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3897 - val_loss: 0.2872\n",
            "Epoch 31/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4153 - val_loss: 0.2687\n",
            "Epoch 32/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3935 - val_loss: 0.2502\n",
            "Epoch 33/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3856 - val_loss: 0.2382\n",
            "Epoch 34/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3572 - val_loss: 0.2300\n",
            "Epoch 35/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3872 - val_loss: 0.2258\n",
            "Epoch 36/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3792 - val_loss: 0.2295\n",
            "Epoch 37/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3357 - val_loss: 0.2210\n",
            "Epoch 38/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3654 - val_loss: 0.2097\n",
            "Epoch 39/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3458 - val_loss: 0.2038\n",
            "Epoch 40/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3649 - val_loss: 0.2061\n",
            "Epoch 41/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3290 - val_loss: 0.2031\n",
            "Epoch 42/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3440 - val_loss: 0.1988\n",
            "Epoch 43/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2927 - val_loss: 0.1920\n",
            "Epoch 44/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3178 - val_loss: 0.1797\n",
            "Epoch 45/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3440 - val_loss: 0.1803\n",
            "Epoch 46/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3077 - val_loss: 0.1767\n",
            "Epoch 47/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2871 - val_loss: 0.1731\n",
            "Epoch 48/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3079 - val_loss: 0.1689\n",
            "Epoch 49/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2699 - val_loss: 0.1656\n",
            "Epoch 50/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3323 - val_loss: 0.1630\n",
            "Epoch 51/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3261 - val_loss: 0.1655\n",
            "Epoch 52/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2729 - val_loss: 0.1818\n",
            "Epoch 53/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3088 - val_loss: 0.1611\n",
            "Epoch 54/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3086 - val_loss: 0.1544\n",
            "Epoch 55/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2797 - val_loss: 0.1508\n",
            "Epoch 56/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2347 - val_loss: 0.1464\n",
            "Epoch 57/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2860 - val_loss: 0.1420\n",
            "Epoch 58/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2550 - val_loss: 0.1366\n",
            "Epoch 59/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2648 - val_loss: 0.1320\n",
            "Epoch 60/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2566 - val_loss: 0.1299\n",
            "Epoch 61/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2716 - val_loss: 0.1321\n",
            "Epoch 62/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2780 - val_loss: 0.1252\n",
            "Epoch 63/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2621 - val_loss: 0.1226\n",
            "Epoch 64/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2633 - val_loss: 0.1199\n",
            "Epoch 65/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2640 - val_loss: 0.1162\n",
            "Epoch 66/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2808 - val_loss: 0.1176\n",
            "Epoch 67/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2406 - val_loss: 0.1182\n",
            "Epoch 68/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2735 - val_loss: 0.1258\n",
            "Epoch 69/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2212 - val_loss: 0.1140\n",
            "Epoch 70/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2371 - val_loss: 0.1142\n",
            "Epoch 71/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2513 - val_loss: 0.1071\n",
            "Epoch 72/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2109 - val_loss: 0.1050\n",
            "Epoch 73/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2094 - val_loss: 0.0997\n",
            "Epoch 74/600\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2568 - val_loss: 0.0971\n",
            "Epoch 75/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2336 - val_loss: 0.0958\n",
            "Epoch 76/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2176 - val_loss: 0.0971\n",
            "Epoch 77/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 0.0969\n",
            "Epoch 78/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2328 - val_loss: 0.0955\n",
            "Epoch 79/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 0.0960\n",
            "Epoch 80/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.1985 - val_loss: 0.0935\n",
            "Epoch 81/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2148 - val_loss: 0.1150\n",
            "Epoch 82/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2238 - val_loss: 0.0932\n",
            "Epoch 83/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2392 - val_loss: 0.0897\n",
            "Epoch 84/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2155 - val_loss: 0.0819\n",
            "Epoch 85/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2394 - val_loss: 0.1553\n",
            "Epoch 86/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.1888 - val_loss: 0.0820\n",
            "Epoch 87/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2122 - val_loss: 0.0813\n",
            "Epoch 88/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2198 - val_loss: 0.0884\n",
            "Epoch 89/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.1795 - val_loss: 0.0842\n",
            "Epoch 90/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.1651 - val_loss: 0.0841\n",
            "Epoch 91/600\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1966 - val_loss: 0.0884\n",
            "Epoch 92/600\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.1944 - val_loss: 0.0878\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fefb26a07f0>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=600, validation_data=(X_test, y_test), callbacks=[earlyStop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "kyHY71AmmR_8",
        "outputId": "926354b8-8d54-4b23-ad28-c2dcc5bbb62b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "lossDf = pd.DataFrame(model.history.history)\n",
        "lossDf.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MPLs3S7xmdgl"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = (predictions > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.96      0.95        72\n",
            "           1       0.97      0.96      0.97       116\n",
            "\n",
            "    accuracy                           0.96       188\n",
            "   macro avg       0.95      0.96      0.96       188\n",
            "weighted avg       0.96      0.96      0.96       188\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
